{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from score import report_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"fnc-1\"\n",
    "w2v_path = './data/GoogleNews-vectors-negative300.bin'\n",
    "save_path = \"./saved/\"\n",
    "batch_size = 128\n",
    "max_sent_length = 250\n",
    "random_state = 37\n",
    "lstm_hidden_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_bodies = pd.read_csv(datadir + '/train_bodies.csv')   \n",
    "raw_train_stances = pd.read_csv(datadir + '/train_stances.csv')\n",
    "raw_test_bodies = pd.read_csv(datadir + '/competition_test_bodies.csv') \n",
    "raw_test_stances = pd.read_csv(datadir + '/competition_test_stances.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36545"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_train_stances['Stance']).count('unrelated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_to_int = {\"agree\":0, \"discuss\": 1, \"disagree\": 2, \"unrelated\": 3}\n",
    "int_to_stance = {0:\"agree\", 1:\"discuss\", 2:\"disagree\", 3: \"unrelated\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_stances['Stance'] = raw_train_stances['Stance'].apply(lambda x: stance_to_int[x])\n",
    "raw_test_stances['Stance'] = raw_test_stances['Stance'].apply(lambda x: stance_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train_stances.join(raw_train_bodies.set_index('Body ID'), on='Body ID')\n",
    "test_df = raw_test_stances.join(raw_test_bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "# Pre-processing words\n",
    "clean_train_headline = [text_to_word_sequence(clean(head)) for head in train_df['Headline']]\n",
    "clean_train_bodies = [text_to_word_sequence(clean(body)) for body in train_df['articleBody']]\n",
    "clean_test_headline = [text_to_word_sequence(clean(head)) for head in test_df['Headline']]\n",
    "clean_test_bodies = [text_to_word_sequence(clean(body)) for body in test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for i in range(len(clean_train_headline)):\n",
    "    wordlist.append(clean_train_headline[i])\n",
    "for i in range(len(clean_train_bodies)):\n",
    "    wordlist.append(clean_train_bodies[i])\n",
    "for i in range(len(clean_test_headline)):\n",
    "    wordlist.append(clean_test_headline[i])\n",
    "for i in range(len(clean_test_bodies)):\n",
    "    wordlist.append(clean_test_bodies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29451"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordlist)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [] \n",
    "for i in range(len(clean_train_headline)):\n",
    "    headline =  clean_train_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    train_lines.append(newline)\n",
    "\n",
    "test_lines = [] \n",
    "for i in range(len(clean_test_headline)):\n",
    "    headline =  clean_test_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in train_lines])\n",
    "raw_X_train = pad_sequences(X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "raw_y_train = train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in test_lines])\n",
    "X_test = pad_sequences(X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "y_test = test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "y_train_onehot = np_utils.to_categorical(raw_y_train)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vali, y_train, y_vali = train_test_split(raw_X_train, y_train_onehot, random_state = random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "del embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(n_classes):\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    num_filters=[80, 80, 80]  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights = [embeddings_matrix],\n",
    "                            trainable=False, name='embedding_layer',\n",
    "                            mask_zero=True))\n",
    "    model.add(keras.layers.Conv1D(num_filters[0], kernel_sizes[0], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_1'))\n",
    "    model.add(keras.layers.MaxPooling1D(3))\n",
    "    model.add(keras.layers.Conv1D(num_filters[1], kernel_sizes[1], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_2'))\n",
    "    model.add(keras.layers.MaxPooling1D(3))\n",
    "    model.add(keras.layers.Conv1D(num_filters[2], kernel_sizes[2], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_3'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax', name='output_layer'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model trained over to four-classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = cnn_model(n_classes=4)\n",
    "basic_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, None, 300)         8835600   \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, None, 80)          72080     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, None, 80)          25680     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, None, 80)          32080     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 324       \n",
      "=================================================================\n",
      "Total params: 8,965,764\n",
      "Trainable params: 130,164\n",
      "Non-trainable params: 8,835,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(basic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 15s 12ms/step - loss: 0.8869 - accuracy: 0.7385 - val_loss: 0.7596 - val_accuracy: 0.7437\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6829 - accuracy: 0.7813 - val_loss: 0.6468 - val_accuracy: 0.7882\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5981 - accuracy: 0.8143 - val_loss: 0.5931 - val_accuracy: 0.8187\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5461 - accuracy: 0.8384 - val_loss: 0.5725 - val_accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5071 - accuracy: 0.8589 - val_loss: 0.5376 - val_accuracy: 0.8507\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4787 - accuracy: 0.8726 - val_loss: 0.5308 - val_accuracy: 0.8541\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4623 - accuracy: 0.8801 - val_loss: 0.5074 - val_accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4431 - accuracy: 0.8898 - val_loss: 0.5328 - val_accuracy: 0.8538\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4347 - accuracy: 0.8927 - val_loss: 0.5053 - val_accuracy: 0.8725\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4147 - accuracy: 0.9029 - val_loss: 0.5033 - val_accuracy: 0.8714\n",
      "INFO:tensorflow:Assets written to: ./saved/basic_mode\\assets\n"
     ]
    }
   ],
   "source": [
    "history = basic_model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(X_vali, y_vali))\n",
    "basic_model.save(save_path+\"basic_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    240    |     0     |    141    |   1522    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    78     |     0     |    58     |    561    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    398    |     1     |    429    |   3636    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |   1827    |     0     |   1751    |   14771   |\n",
      "-------------------------------------------------------------\n",
      "Score: 4530.75 out of 11651.25\t(38.88638558094625%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.88638558094625"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = basic_model.predict(X_test)\n",
    "outputs = [int_to_stance[np.argmax(p, axis = -1)] for p in preds]\n",
    "report_score(actual_test_stances,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relatedness Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agree, disagree, discuss are 1, unrelated is 0\n",
    "int_to_relatedness={0:1,1:1,2:1,3:0}\n",
    "str_to_relatedness = {'unrelated':0 , 'related':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_y_train = raw_y_train.copy()\n",
    "relatedness_y_train = relatedness_y_train.apply(lambda x: int_to_relatedness[x])\n",
    "relatedness_y_train_onehot = np_utils.to_categorical(relatedness_y_train)\n",
    "\n",
    "relatedness_y_test = y_test.copy()\n",
    "relatedness_y_test = relatedness_y_test.apply(lambda x: int_to_relatedness[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_X_train, relatedness_X_vali, relatedness_y_train, relatedness_y_vali = train_test_split(raw_X_train, relatedness_y_train_onehot, random_state=random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_model = cnn_model(n_classes=2)\n",
    "relatedness_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5060 - accuracy: 0.8147 - val_loss: 0.4832 - val_accuracy: 0.8262\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.4291 - accuracy: 0.8539 - val_loss: 0.4382 - val_accuracy: 0.8449\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3808 - accuracy: 0.8824 - val_loss: 0.4024 - val_accuracy: 0.8663\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3568 - accuracy: 0.8951 - val_loss: 0.3923 - val_accuracy: 0.8810\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3366 - accuracy: 0.9059 - val_loss: 0.3875 - val_accuracy: 0.8836\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3188 - accuracy: 0.9156 - val_loss: 0.3703 - val_accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3058 - accuracy: 0.9237 - val_loss: 0.3550 - val_accuracy: 0.9017\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3007 - accuracy: 0.9261 - val_loss: 0.3508 - val_accuracy: 0.9026\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.2880 - accuracy: 0.9327 - val_loss: 0.3809 - val_accuracy: 0.8947\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.2826 - accuracy: 0.9354 - val_loss: 0.3495 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "history = relatedness_model.fit(X_train, y_train,batch_size=batch_size,epochs=10, validation_data=(relatedness_X_vali, relatedness_y_vali))\n",
    "preds = relatedness_model.predict(X_test)\n",
    "outputs = [np.argmax(p, axis = -1) for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6561602329516389"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(relatedness_y_test, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinion Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset that all unrelated column is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_index = train_df[train_df['Stance']==3].index\n",
    "opinion_train_df = train_df.drop(train_df[train_df['Stance']==3].index)\n",
    "\n",
    "test_drop_index = test_df[test_df['Stance']==3].index\n",
    "opinion_test_df = test_df.drop(test_df[test_df['Stance']==3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7064"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train_headline = [text_to_word_sequence(clean(head)) for head in opinion_train_df['Headline']]\n",
    "opinion_train_bodies = [text_to_word_sequence(clean(body)) for body in opinion_train_df['articleBody']]\n",
    "\n",
    "opinion_test_headline = [text_to_word_sequence(clean(head)) for head in opinion_test_df['Headline']]\n",
    "opinion_test_bodies = [text_to_word_sequence(clean(body)) for body in opinion_test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train_lines = [] \n",
    "for i in range(len(opinion_train_headline)):\n",
    "    headline =  opinion_train_bodies[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    opinion_train_lines.append(newline)\n",
    "\n",
    "opinion_test_lines = [] \n",
    "for i in range(len(opinion_test_headline)):\n",
    "    headline =  opinion_test_bodies[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    opinion_test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13427"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in opinion_train_lines])\n",
    "opinion_raw_X_train = pad_sequences(opinion_X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "opinion_raw_y_train = opinion_train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in opinion_test_lines])\n",
    "opinion_raw_X_test = pad_sequences(opinion_X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "opinion_y_test = opinion_test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "opinion_y_train_onehot = np_utils.to_categorical(opinion_raw_y_train)\n",
    "opinion_y_test_onehot = np_utils.to_categorical(opinion_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_train, opinion_X_vali, opinion_y_train, opinion_y_vali = train_test_split(opinion_raw_X_train, opinion_y_train_onehot, random_state=random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_model = cnn_model(n_classes=3)\n",
    "opinion_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "84/84 [==============================] - 2s 15ms/step - loss: 0.9927 - accuracy: 0.6758 - val_loss: 0.8806 - val_accuracy: 0.6910\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8198 - accuracy: 0.7397 - val_loss: 0.7581 - val_accuracy: 0.7621\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7243 - accuracy: 0.7723 - val_loss: 0.7157 - val_accuracy: 0.7829\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6698 - accuracy: 0.7902 - val_loss: 0.7271 - val_accuracy: 0.7766\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6390 - accuracy: 0.7935 - val_loss: 0.6559 - val_accuracy: 0.7889\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5970 - accuracy: 0.8104 - val_loss: 0.6172 - val_accuracy: 0.7964\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5789 - accuracy: 0.8142 - val_loss: 0.6532 - val_accuracy: 0.7867\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5683 - accuracy: 0.8162 - val_loss: 0.6005 - val_accuracy: 0.8049\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5378 - accuracy: 0.8282 - val_loss: 0.6018 - val_accuracy: 0.7904\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5256 - accuracy: 0.8292 - val_loss: 0.6191 - val_accuracy: 0.8031\n",
      "INFO:tensorflow:Assets written to: ./saved/opinion_model\\assets\n"
     ]
    }
   ],
   "source": [
    "history = opinion_model.fit(opinion_X_train, opinion_y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(opinion_X_vali, opinion_y_vali))\n",
    "basic_model.save(save_path+\"opinion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5549263873159683"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = opinion_model.predict(opinion_raw_X_test)\n",
    "outputs = [np.argmax(p, axis = -1) for p in preds]\n",
    "accuracy_score(opinion_y_test, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Relatedness model and Opinion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_model:\n",
    "    def __init__(self,relatedness_model, opinion_model):\n",
    "        self.relatedness_model = relatedness_model\n",
    "        self.opinion_model = opinion_model\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        prediction = relatedness_model.predict(X_test)\n",
    "        prediction = [np.argmax(p, axis = -1) for p in prediction]\n",
    "        for i in tqdm(range(len(prediction))):\n",
    "            relatedness = prediction[i]\n",
    "            if relatedness == 1: #related\n",
    "                opinion = opinion_model.predict(np.array([X_test[i]]))\n",
    "                opinion = np.argmax(opinion, axis = -1)\n",
    "                \n",
    "                prediction[i] = int_to_stance[int(opinion)]\n",
    "            else:\n",
    "                prediction[i] = 'unrelated'\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 696/2686 [00:13<00:39, 50.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kai\\Desktop\\MSCI641_fake_news_challenge\\CNN.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000047?line=0'>1</a>\u001b[0m combined \u001b[39m=\u001b[39m combined_model(relatedness_model, opinion_model)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000047?line=1'>2</a>\u001b[0m outputs \u001b[39m=\u001b[39m combined\u001b[39m.\u001b[39;49mpredict(opinion_X_vali)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000047?line=2'>3</a>\u001b[0m report_score(actual_test_stances,outputs)\n",
      "\u001b[1;32mc:\\Users\\Kai\\Desktop\\MSCI641_fake_news_challenge\\CNN.ipynb Cell 47'\u001b[0m in \u001b[0;36mcombined_model.predict\u001b[1;34m(self, X_test)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000046?line=9'>10</a>\u001b[0m relatedness \u001b[39m=\u001b[39m prediction[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000046?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m relatedness \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m: \u001b[39m#related\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000046?line=11'>12</a>\u001b[0m     opinion \u001b[39m=\u001b[39m opinion_model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([X_test[i]]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000046?line=12'>13</a>\u001b[0m     opinion \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(opinion, axis \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kai/Desktop/MSCI641_fake_news_challenge/CNN.ipynb#ch0000046?line=14'>15</a>\u001b[0m     prediction[i] \u001b[39m=\u001b[39m int_to_stance[\u001b[39mint\u001b[39m(opinion)]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py:1720\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1713'>1714</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1714'>1715</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1715'>1716</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1716'>1717</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1717'>1718</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1719'>1720</a>\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1720'>1721</a>\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1721'>1722</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1722'>1723</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1723'>1724</a>\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1724'>1725</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1725'>1726</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1726'>1727</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1727'>1728</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1728'>1729</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1729'>1730</a>\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1731'>1732</a>\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/training.py?line=1732'>1733</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1380'>1381</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1381'>1382</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1382'>1383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1134'>1135</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1136'>1137</a>\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1137'>1138</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1138'>1139</a>\u001b[0m     x,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1139'>1140</a>\u001b[0m     y,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1140'>1141</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1141'>1142</a>\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1142'>1143</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1143'>1144</a>\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1144'>1145</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1145'>1146</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1146'>1147</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1147'>1148</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1148'>1149</a>\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1149'>1150</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1151'>1152</a>\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=1153'>1154</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\data_adapter.py:271\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=268'>269</a>\u001b[0m indices_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mrange(\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=269'>270</a>\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=270'>271</a>\u001b[0m   indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mrepeat(epochs)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=272'>273</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpermutation\u001b[39m(_):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=273'>274</a>\u001b[0m   \u001b[39m# It turns out to be more performant to make a new set of indices rather\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=274'>275</a>\u001b[0m   \u001b[39m# than reusing the same range Tensor. (presumably because of buffer\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=275'>276</a>\u001b[0m   \u001b[39m# forwarding.)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/keras/engine/data_adapter.py?line=276'>277</a>\u001b[0m   indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrange(num_samples, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1262\u001b[0m, in \u001b[0;36mDatasetV2.repeat\u001b[1;34m(self, count)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1242'>1243</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat\u001b[39m(\u001b[39mself\u001b[39m, count\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1243'>1244</a>\u001b[0m   \u001b[39m\"\"\"Repeats this dataset so each original value is seen `count` times.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1244'>1245</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1245'>1246</a>\u001b[0m \u001b[39m  >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1259'>1260</a>\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1260'>1261</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=1261'>1262</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m RepeatDataset(\u001b[39mself\u001b[39;49m, count)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4411\u001b[0m, in \u001b[0;36mRepeatDataset.__init__\u001b[1;34m(self, input_dataset, count)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4407'>4408</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4408'>4409</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4409'>4410</a>\u001b[0m       count, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint64, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4410'>4411</a>\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mrepeat_dataset(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4411'>4412</a>\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4412'>4413</a>\u001b[0m     count\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4413'>4414</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_structure)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4414'>4415</a>\u001b[0m \u001b[39msuper\u001b[39m(RepeatDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:5954\u001b[0m, in \u001b[0;36mrepeat_dataset\u001b[1;34m(input_dataset, count, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5951'>5952</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5952'>5953</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5953'>5954</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5954'>5955</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mRepeatDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, count, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5955'>5956</a>\u001b[0m       output_types, \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5956'>5957</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/keras/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=5957'>5958</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "combined = combined_model(relatedness_model, opinion_model)\n",
    "outputs = combined.predict()\n",
    "report_score(actual_test_stances,outputs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8c2f1beb2e40a9a38baaa548f28e428769bf26bad7d7d25cc139235a548e0b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
