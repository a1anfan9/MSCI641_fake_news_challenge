{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from score import report_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"fnc-1\"\n",
    "w2v_path = './w2v/GoogleNews-vectors-negative300.bin'\n",
    "save_path = \"./saved/\"\n",
    "batch_size = 128\n",
    "max_sent_length = 350\n",
    "random_state = 37\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_bodies = pd.read_csv(datadir + '/train_bodies.csv')   \n",
    "raw_train_stances = pd.read_csv(datadir + '/train_stances.csv')\n",
    "raw_test_bodies = pd.read_csv(datadir + '/competition_test_bodies.csv') \n",
    "raw_test_stances = pd.read_csv(datadir + '/competition_test_stances.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_to_int = {\"agree\":0, \"discuss\": 1, \"disagree\": 2, \"unrelated\": 3}\n",
    "int_to_stance = {0:\"agree\", 1:\"discuss\", 2:\"disagree\", 3: \"unrelated\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_test_stances = raw_test_stances['Stance']\n",
    "raw_train_stances['Stance'] = raw_train_stances['Stance'].apply(lambda x: stance_to_int[x])\n",
    "raw_test_stances['Stance'] = raw_test_stances['Stance'].apply(lambda x: stance_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train_stances.join(raw_train_bodies.set_index('Body ID'), on='Body ID')\n",
    "test_df = raw_test_stances.join(raw_test_bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "# Pre-processing words\n",
    "clean_train_headline = [text_to_word_sequence(clean(head)) for head in train_df['Headline']]\n",
    "clean_train_bodies = [text_to_word_sequence(clean(body)) for body in train_df['articleBody']]\n",
    "clean_test_headline = [text_to_word_sequence(clean(head)) for head in test_df['Headline']]\n",
    "clean_test_bodies = [text_to_word_sequence(clean(body)) for body in test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for i in range(len(clean_train_headline)):\n",
    "    wordlist.append(clean_train_headline[i])\n",
    "for i in range(len(clean_train_bodies)):\n",
    "    wordlist.append(clean_train_bodies[i])\n",
    "for i in range(len(clean_test_headline)):\n",
    "    wordlist.append(clean_test_headline[i])\n",
    "for i in range(len(clean_test_bodies)):\n",
    "    wordlist.append(clean_test_bodies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29451"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordlist)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [] \n",
    "for i in range(len(clean_train_headline)):\n",
    "    headline =  clean_train_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    train_lines.append(newline)\n",
    "\n",
    "test_lines = [] \n",
    "for i in range(len(clean_test_headline)):\n",
    "    headline =  clean_test_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in train_lines])\n",
    "raw_X_train = pad_sequences(X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "raw_y_train = train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in test_lines])\n",
    "X_test = pad_sequences(X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "y_test = test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "y_train_onehot = np_utils.to_categorical(raw_y_train)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "del embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(n_classes):\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    num_filters=[80, 80, 80]  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights = [embeddings_matrix],\n",
    "                            trainable=False, name='embedding_layer',\n",
    "                            mask_zero=True))\n",
    "    model.add(keras.layers.Conv1D(num_filters[0], kernel_sizes[0], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_1'))\n",
    "    model.add(keras.layers.MaxPooling1D(3))\n",
    "    model.add(keras.layers.Conv1D(num_filters[1], kernel_sizes[1], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_2'))\n",
    "    model.add(keras.layers.MaxPooling1D(3))\n",
    "    model.add(keras.layers.Conv1D(num_filters[2], kernel_sizes[2], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_3'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax', name='output_layer'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model trained over to Stance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = cnn_model(n_classes=4)\n",
    "basic_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, None, 300)         8835600   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 80)          72080     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 80)          25680     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 80)          32080     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 324       \n",
      "=================================================================\n",
      "Total params: 8,965,764\n",
      "Trainable params: 130,164\n",
      "Non-trainable params: 8,835,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(basic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 12s 12ms/step - loss: 0.8655 - accuracy: 0.7400\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.6660 - accuracy: 0.7892\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.5699 - accuracy: 0.8310\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.5161 - accuracy: 0.8565\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.4847 - accuracy: 0.8709\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.4581 - accuracy: 0.8844\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.4348 - accuracy: 0.8940\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.4217 - accuracy: 0.9004\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.4053 - accuracy: 0.9081\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3900 - accuracy: 0.9155\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3806 - accuracy: 0.9190\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.3678 - accuracy: 0.9233\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.3597 - accuracy: 0.9277\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3537 - accuracy: 0.9308\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3417 - accuracy: 0.9356\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3372 - accuracy: 0.9366\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3297 - accuracy: 0.9392\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3267 - accuracy: 0.9407\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3226 - accuracy: 0.9427\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.3202 - accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "history = basic_model.fit(raw_X_train, y_train_onehot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch,\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    98     |    19     |    116    |   1670    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    37     |     5     |    53     |    602    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    171    |    22     |    375    |   3896    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    823    |    107    |   1337    |   16082   |\n",
      "-------------------------------------------------------------\n",
      "Score: 4603.0 out of 11651.25\t(39.50649071987984%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.50649071987984"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = basic_model.predict(X_test)\n",
    "outputs = [int_to_stance[np.argmax(p, axis = -1)] for p in preds]\n",
    "report_score(actual_test_stances,outputs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8c2f1beb2e40a9a38baaa548f28e428769bf26bad7d7d25cc139235a548e0b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
