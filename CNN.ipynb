{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from score import report_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"fnc-1\"\n",
    "w2v_path = './w2v/GoogleNews-vectors-negative300.bin'\n",
    "save_path = \"./saved/\"\n",
    "batch_size = 128\n",
    "max_sent_length = 350\n",
    "random_state = 37\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_bodies = pd.read_csv(datadir + '/train_bodies.csv')   \n",
    "raw_train_stances = pd.read_csv(datadir + '/train_stances.csv')\n",
    "raw_test_bodies = pd.read_csv(datadir + '/competition_test_bodies.csv') \n",
    "raw_test_stances = pd.read_csv(datadir + '/competition_test_stances.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_to_int = {\"agree\":0, \"discuss\": 1, \"disagree\": 2, \"unrelated\": 3}\n",
    "int_to_stance = {0:\"agree\", 1:\"discuss\", 2:\"disagree\", 3: \"unrelated\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_test_stances = raw_test_stances['Stance']\n",
    "raw_train_stances['Stance'] = raw_train_stances['Stance'].apply(lambda x: stance_to_int[x])\n",
    "raw_test_stances['Stance'] = raw_test_stances['Stance'].apply(lambda x: stance_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train_stances.join(raw_train_bodies.set_index('Body ID'), on='Body ID')\n",
    "test_df = raw_test_stances.join(raw_test_bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "# Pre-processing words\n",
    "clean_train_headline = [text_to_word_sequence(clean(head)) for head in train_df['Headline']]\n",
    "clean_train_bodies = [text_to_word_sequence(clean(body)) for body in train_df['articleBody']]\n",
    "clean_test_headline = [text_to_word_sequence(clean(head)) for head in test_df['Headline']]\n",
    "clean_test_bodies = [text_to_word_sequence(clean(body)) for body in test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for i in range(len(clean_train_headline)):\n",
    "    wordlist.append(clean_train_headline[i])\n",
    "for i in range(len(clean_train_bodies)):\n",
    "    wordlist.append(clean_train_bodies[i])\n",
    "for i in range(len(clean_test_headline)):\n",
    "    wordlist.append(clean_test_headline[i])\n",
    "for i in range(len(clean_test_bodies)):\n",
    "    wordlist.append(clean_test_bodies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29451"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordlist)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [] \n",
    "for i in range(len(clean_train_headline)):\n",
    "    headline =  clean_train_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    train_lines.append(newline)\n",
    "\n",
    "test_lines = [] \n",
    "for i in range(len(clean_test_headline)):\n",
    "    headline =  clean_test_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in train_lines])\n",
    "raw_X_train = pad_sequences(X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "raw_y_train = train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in test_lines])\n",
    "X_test = pad_sequences(X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "y_test = test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vali, y_train, y_vali = train_test_split(raw_X_train, raw_y_train, random_state = random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "y_train_onehot = np_utils.to_categorical(y_train)\n",
    "y_vali_onehot = np_utils.to_categorical(y_vali)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "del embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(n_classes):\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    num_filters=[80, 80, 80]  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights = [embeddings_matrix],\n",
    "                            trainable=False, name='embedding_layer',\n",
    "                            mask_zero=True))\n",
    "    model.add(keras.layers.Conv1D(num_filters[0], kernel_sizes[0], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_1'))\n",
    "    model.add(keras.layers.MaxPooling1D(3))\n",
    "    model.add(keras.layers.Conv1D(num_filters[1], kernel_sizes[1], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_2'))\n",
    "    model.add(keras.layers.MaxPooling1D(3))\n",
    "    model.add(keras.layers.Conv1D(num_filters[2], kernel_sizes[2], padding='valid', activation='relu', kernel_regularizer=regularizers.L2(0.001)))\n",
    "    model.add(Activation(activation='relu', name='activation_3'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax', name='output_layer'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model trained over to Stance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = cnn_model(n_classes=4)\n",
    "basic_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, None, 300)         8835600   \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, None, 80)          72080     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, None, 80)          25680     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, None, 80)          32080     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 80)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_19 (Glo (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 324       \n",
      "=================================================================\n",
      "Total params: 8,965,764\n",
      "Trainable params: 130,164\n",
      "Non-trainable params: 8,835,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(basic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.8955 - accuracy: 0.7377 - val_loss: 0.7742 - val_accuracy: 0.7560\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6874 - accuracy: 0.7770 - val_loss: 0.6350 - val_accuracy: 0.7964\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5830 - accuracy: 0.8228 - val_loss: 0.5711 - val_accuracy: 0.8334\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5289 - accuracy: 0.8494 - val_loss: 0.5623 - val_accuracy: 0.8402\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4900 - accuracy: 0.8690 - val_loss: 0.5328 - val_accuracy: 0.8492\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4640 - accuracy: 0.8802 - val_loss: 0.5173 - val_accuracy: 0.8585\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4452 - accuracy: 0.8901 - val_loss: 0.5084 - val_accuracy: 0.8654\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4260 - accuracy: 0.8978 - val_loss: 0.4980 - val_accuracy: 0.8715\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4127 - accuracy: 0.9045 - val_loss: 0.5523 - val_accuracy: 0.8517\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4047 - accuracy: 0.9095 - val_loss: 0.4960 - val_accuracy: 0.8743\n",
      "INFO:tensorflow:Assets written to: ./saved/basic_mode\\assets\n"
     ]
    }
   ],
   "source": [
    "history = basic_model.fit(X_train, y_train_onehot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch,\n",
    "          validation_data=(X_vali, y_vali_onehot))\n",
    "basic_model.save(save_path+\"basic_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Relatedness Accuracy is 0.895847923961981\n",
      "The Opinion Accuracy is 0.6226277372262774\n"
     ]
    }
   ],
   "source": [
    "# Accuracies on the validation set\n",
    "preds_vali = np.argmax(basic_model.predict(X_vali), axis = -1)\n",
    "true_valid = np.argmax(y_vali_onehot, axis = -1)\n",
    "\n",
    "total_relatedness = len(true_valid)\n",
    "total_opinion = 0\n",
    "correct_relatedness = 0\n",
    "correct_opinion = 0\n",
    "\n",
    "for i in range(len(true_valid)):\n",
    "    true = true_valid[i]\n",
    "    pred = preds_vali[i]\n",
    "    if true==3:\n",
    "        if pred==3:\n",
    "            correct_relatedness+=1\n",
    "    else:\n",
    "        total_opinion+=1\n",
    "        if pred!=3:\n",
    "            correct_relatedness+=1\n",
    "        if pred==true:\n",
    "            correct_opinion+=1\n",
    "\n",
    "print(f\"The Relatedness Accuracy is {correct_relatedness/total_relatedness}\")\n",
    "print(f\"The Opinion Accuracy is {correct_opinion/total_opinion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    174    |     9     |    101    |   1619    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    54     |     5     |    33     |    605    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    255    |    19     |    314    |   3876    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |   1321    |    87     |   1184    |   15757   |\n",
      "-------------------------------------------------------------\n",
      "Score: 4550.0 out of 11651.25\t(39.05160390516039%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.05160390516039"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = basic_model.predict(X_test)\n",
    "outputs = [int_to_stance[np.argmax(p, axis = -1)] for p in preds]\n",
    "report_score(actual_test_stances,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relatedness Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agree, disagree, discuss are 1, unrelated is 0\n",
    "int_to_relatedness={0:1,1:1,2:1,3:0}\n",
    "str_to_relatedness = {'unrelated':0 , 'related':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_y_train = y_train.apply(lambda x: int_to_relatedness[x])\n",
    "relatedness_y_train_onehot = np_utils.to_categorical(relatedness_y_train)\n",
    "\n",
    "relatedness_y_vali = y_vali.apply(lambda x: int_to_relatedness[x])\n",
    "relatedness_y_vali_onehot = np_utils.to_categorical(relatedness_y_vali)\n",
    "\n",
    "relatedness_y_test = y_test.copy()\n",
    "relatedness_y_test = relatedness_y_test.apply(lambda x: int_to_relatedness[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_model = cnn_model(n_classes=2)\n",
    "relatedness_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6703 - accuracy: 0.7421 - val_loss: 0.5516 - val_accuracy: 0.7972\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4994 - accuracy: 0.8156 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4276 - accuracy: 0.8548 - val_loss: 0.4298 - val_accuracy: 0.8489\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3852 - accuracy: 0.8759 - val_loss: 0.4380 - val_accuracy: 0.8511\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3590 - accuracy: 0.8929 - val_loss: 0.4006 - val_accuracy: 0.8679\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3363 - accuracy: 0.9052 - val_loss: 0.3758 - val_accuracy: 0.8892\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3208 - accuracy: 0.9139 - val_loss: 0.3701 - val_accuracy: 0.8915\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3069 - accuracy: 0.9211 - val_loss: 0.3935 - val_accuracy: 0.8819\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2968 - accuracy: 0.9268 - val_loss: 0.3585 - val_accuracy: 0.8955\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2849 - accuracy: 0.9327 - val_loss: 0.3559 - val_accuracy: 0.9012\n"
     ]
    }
   ],
   "source": [
    "history = relatedness_model.fit(X_train, relatedness_y_train_onehot ,batch_size=batch_size,epochs=epoch, \n",
    "                                validation_data=(X_vali, relatedness_y_vali_onehot))\n",
    "preds = relatedness_model.predict(X_test)\n",
    "outputs = [np.argmax(p, axis = -1) for p in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinion Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset that all unrelated column is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_index = train_df[train_df['Stance']==3].index\n",
    "opinion_train_df = train_df.drop(train_df[train_df['Stance']==3].index)\n",
    "\n",
    "test_drop_index = test_df[test_df['Stance']==3].index\n",
    "opinion_test_df = test_df.drop(test_df[test_df['Stance']==3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train_headline = [text_to_word_sequence(clean(head)) for head in opinion_train_df['Headline']]\n",
    "opinion_train_bodies = [text_to_word_sequence(clean(body)) for body in opinion_train_df['articleBody']]\n",
    "\n",
    "opinion_test_headline = [text_to_word_sequence(clean(head)) for head in opinion_test_df['Headline']]\n",
    "opinion_test_bodies = [text_to_word_sequence(clean(body)) for body in opinion_test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train_lines = [] \n",
    "for i in range(len(opinion_train_headline)):\n",
    "    headline =  opinion_train_bodies[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    opinion_train_lines.append(newline)\n",
    "\n",
    "opinion_test_lines = [] \n",
    "for i in range(len(opinion_test_headline)):\n",
    "    headline =  opinion_test_bodies[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    opinion_test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13427"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in opinion_train_lines])\n",
    "opinion_raw_X_train = pad_sequences(opinion_X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "opinion_raw_y_train = opinion_train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in opinion_test_lines])\n",
    "opinion_raw_X_test = pad_sequences(opinion_X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "opinion_y_test = opinion_test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "opinion_y_train_onehot = np_utils.to_categorical(opinion_raw_y_train)\n",
    "opinion_y_test_onehot = np_utils.to_categorical(opinion_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_train, opinion_X_vali, opinion_y_train, opinion_y_vali = train_test_split(opinion_raw_X_train, opinion_y_train_onehot, random_state=random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_model = cnn_model(n_classes=3)\n",
    "opinion_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "84/84 [==============================] - 2s 14ms/step - loss: 1.0053 - accuracy: 0.6661 - val_loss: 0.9053 - val_accuracy: 0.7200\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.8440 - accuracy: 0.7241 - val_loss: 0.8178 - val_accuracy: 0.7398\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.7688 - accuracy: 0.7539 - val_loss: 0.7405 - val_accuracy: 0.7599\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.7152 - accuracy: 0.7686 - val_loss: 0.7310 - val_accuracy: 0.7789\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.6806 - accuracy: 0.7834 - val_loss: 0.6759 - val_accuracy: 0.7755\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6334 - accuracy: 0.8001 - val_loss: 0.6718 - val_accuracy: 0.7867\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.6021 - accuracy: 0.8087 - val_loss: 0.7704 - val_accuracy: 0.7368\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5815 - accuracy: 0.8181 - val_loss: 0.6419 - val_accuracy: 0.7889\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5691 - accuracy: 0.8255 - val_loss: 0.6680 - val_accuracy: 0.7822\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.5371 - accuracy: 0.8336 - val_loss: 0.6570 - val_accuracy: 0.7915\n"
     ]
    }
   ],
   "source": [
    "history = opinion_model.fit(opinion_X_train, opinion_y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch,\n",
    "          validation_data=(opinion_X_vali, opinion_y_vali))\n",
    "#basic_model.save(save_path+\"opinion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5863533408833522"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = opinion_model.predict(opinion_raw_X_test)\n",
    "outputs = [np.argmax(p, axis = -1) for p in preds]\n",
    "accuracy_score(opinion_y_test, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Relatedness model and Opinion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cascaded_model:\n",
    "    def __init__(self,relatedness_model, opinion_model):\n",
    "        self.relatedness_model = relatedness_model\n",
    "        self.opinion_model = opinion_model\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        prediction = relatedness_model.predict(X_test)\n",
    "        prediction = [np.argmax(p, axis = -1) for p in prediction]\n",
    "        for i in tqdm(range(len(prediction))):\n",
    "            relatedness = prediction[i]\n",
    "            if relatedness == 1: #related\n",
    "                opinion = opinion_model.predict(np.array([X_test[i]]))\n",
    "                opinion = np.argmax(opinion, axis = -1)\n",
    "                \n",
    "                prediction[i] = int(opinion)\n",
    "            else:\n",
    "                prediction[i] = 3\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9995/9995 [01:19<00:00, 126.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stance accuracy of cascaded model is 0.8604302151075538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cascaded = cascaded_model(relatedness_model, opinion_model)\n",
    "preds_vali = cascaded.predict(X_vali)\n",
    "true_valid = np.argmax(y_vali_onehot, axis = -1)\n",
    "print(f\"The stance accuracy of cascaded model is {accuracy_score(true_valid,preds_vali)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8c2f1beb2e40a9a38baaa548f28e428769bf26bad7d7d25cc139235a548e0b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
