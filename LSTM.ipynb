{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from score import report_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"fnc-1\"\n",
    "w2v_path = './w2v/GoogleNews-vectors-negative300.bin'\n",
    "save_path = \"./saved/\"\n",
    "batch_size = 128\n",
    "max_sent_length = 250\n",
    "random_state = 37\n",
    "lstm_hidden_dim = 100\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_bodies = pd.read_csv(datadir + '/train_bodies.csv')   \n",
    "raw_train_stances = pd.read_csv(datadir + '/train_stances.csv')\n",
    "raw_test_bodies = pd.read_csv(datadir + '/competition_test_bodies.csv') \n",
    "raw_test_stances = pd.read_csv(datadir + '/competition_test_stances.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_to_int = {\"agree\":0, \"discuss\": 1, \"disagree\": 2, \"unrelated\": 3}\n",
    "int_to_stance = {0:\"agree\", 1:\"discuss\", 2:\"disagree\", 3: \"unrelated\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_test_stances = raw_test_stances['Stance']\n",
    "raw_train_stances['Stance'] = raw_train_stances['Stance'].apply(lambda x: stance_to_int[x])\n",
    "raw_test_stances['Stance'] = raw_test_stances['Stance'].apply(lambda x: stance_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train_stances.join(raw_train_bodies.set_index('Body ID'), on='Body ID')\n",
    "test_df = raw_test_stances.join(raw_test_bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "# Pre-processing words\n",
    "clean_train_headline = [text_to_word_sequence(clean(head)) for head in train_df['Headline']]\n",
    "clean_train_bodies = [text_to_word_sequence(clean(body)) for body in train_df['articleBody']]\n",
    "clean_test_headline = [text_to_word_sequence(clean(head)) for head in test_df['Headline']]\n",
    "clean_test_bodies = [text_to_word_sequence(clean(body)) for body in test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for i in range(len(clean_train_headline)):\n",
    "    wordlist.append(clean_train_headline[i])\n",
    "for i in range(len(clean_train_bodies)):\n",
    "    wordlist.append(clean_train_bodies[i])\n",
    "for i in range(len(clean_test_headline)):\n",
    "    wordlist.append(clean_test_headline[i])\n",
    "for i in range(len(clean_test_bodies)):\n",
    "    wordlist.append(clean_test_bodies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29451"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordlist)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [] \n",
    "for i in range(len(clean_train_headline)):\n",
    "    headline =  clean_train_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    train_lines.append(newline)\n",
    "\n",
    "test_lines = [] \n",
    "for i in range(len(clean_test_headline)):\n",
    "    headline =  clean_test_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in train_lines])\n",
    "raw_X_train = pad_sequences(X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "raw_y_train = train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in test_lines])\n",
    "X_test = pad_sequences(X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "y_test = test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "y_train_onehot = np_utils.to_categorical(raw_y_train)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "del embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights = [embeddings_matrix],\n",
    "                            trainable=False, name='embedding_layer',\n",
    "                            mask_zero=True))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(lstm_hidden_dim, return_sequences=False, name='lstm_layer',\n",
    "                    \n",
    "                    kernel_regularizer =tf.keras.regularizers.L2(l2=1e-3))))\n",
    "    model.add(Dropout(rate=0.8, name='dropout'))\n",
    "    model.add(Activation(activation='relu', name='activation_1'))\n",
    "    model.add(Dense(n_classes, activation='softmax', name='output_layer'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model trained over to four-classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = lstm_model(n_classes=4)\n",
    "basic_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, None, 300)         8835600   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 9,157,204\n",
      "Trainable params: 321,604\n",
      "Non-trainable params: 8,835,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(basic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 29s 52ms/step - loss: 0.9062 - accuracy: 0.7330\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.7229 - accuracy: 0.7497\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6388 - accuracy: 0.7738\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6031 - accuracy: 0.7884\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5625 - accuracy: 0.8050\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.5515 - accuracy: 0.8119\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.5284 - accuracy: 0.8226\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.5050 - accuracy: 0.8311\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.4871 - accuracy: 0.8387\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.4715 - accuracy: 0.8454\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4803 - accuracy: 0.8434\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4439 - accuracy: 0.8571\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.4405 - accuracy: 0.8594\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.4228 - accuracy: 0.8672\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4235 - accuracy: 0.8672\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.4584 - accuracy: 0.8589\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4371 - accuracy: 0.8686\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.3994 - accuracy: 0.8764\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.3836 - accuracy: 0.8853\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.4057 - accuracy: 0.8805\n"
     ]
    }
   ],
   "source": [
    "history = basic_model.fit(raw_X_train, y_train_onehot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch)\n",
    "# basic_model.save(save_path+\"basic_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    65     |     0     |    53     |   1785    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    16     |     0     |    19     |    662    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    89     |     0     |    278    |   4097    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    471    |     0     |    940    |   16938   |\n",
      "-------------------------------------------------------------\n",
      "Score: 4621.75 out of 11651.25\t(39.667417659049455%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.667417659049455"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = basic_model.predict(X_test)\n",
    "outputs = [int_to_stance[np.argmax(p, axis = -1)] for p in preds]\n",
    "report_score(actual_test_stances,outputs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8c2f1beb2e40a9a38baaa548f28e428769bf26bad7d7d25cc139235a548e0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
