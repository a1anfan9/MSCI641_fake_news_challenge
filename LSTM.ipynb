{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from score import report_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"fnc-1\"\n",
    "w2v_path = './data/GoogleNews-vectors-negative300.bin'\n",
    "save_path = \"./saved/\"\n",
    "batch_size = 128\n",
    "max_sent_length = 250\n",
    "random_state = 37\n",
    "lstm_hidden_dim = 100\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_bodies = pd.read_csv(datadir + '/train_bodies.csv')   \n",
    "raw_train_stances = pd.read_csv(datadir + '/train_stances.csv')\n",
    "raw_test_bodies = pd.read_csv(datadir + '/competition_test_bodies.csv') \n",
    "raw_test_stances = pd.read_csv(datadir + '/competition_test_stances.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_to_int = {\"agree\":0, \"discuss\": 1, \"disagree\": 2, \"unrelated\": 3}\n",
    "int_to_stance = {0:\"agree\", 1:\"discuss\", 2:\"disagree\", 3: \"unrelated\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_test_stances = raw_test_stances['Stance']\n",
    "raw_train_stances['Stance'] = raw_train_stances['Stance'].apply(lambda x: stance_to_int[x])\n",
    "raw_test_stances['Stance'] = raw_test_stances['Stance'].apply(lambda x: stance_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train_stances.join(raw_train_bodies.set_index('Body ID'), on='Body ID')\n",
    "test_df = raw_test_stances.join(raw_test_bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "# Pre-processing words\n",
    "clean_train_headline = [text_to_word_sequence(clean(head)) for head in train_df['Headline']]\n",
    "clean_train_bodies = [text_to_word_sequence(clean(body)) for body in train_df['articleBody']]\n",
    "clean_test_headline = [text_to_word_sequence(clean(head)) for head in test_df['Headline']]\n",
    "clean_test_bodies = [text_to_word_sequence(clean(body)) for body in test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for i in range(len(clean_train_headline)):\n",
    "    wordlist.append(clean_train_headline[i])\n",
    "for i in range(len(clean_train_bodies)):\n",
    "    wordlist.append(clean_train_bodies[i])\n",
    "for i in range(len(clean_test_headline)):\n",
    "    wordlist.append(clean_test_headline[i])\n",
    "for i in range(len(clean_test_bodies)):\n",
    "    wordlist.append(clean_test_bodies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29451"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordlist)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [] \n",
    "for i in range(len(clean_train_headline)):\n",
    "    headline =  clean_train_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    train_lines.append(newline)\n",
    "\n",
    "test_lines = [] \n",
    "for i in range(len(clean_test_headline)):\n",
    "    headline =  clean_test_headline[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in train_lines])\n",
    "raw_X_train = pad_sequences(X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "raw_y_train = train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in test_lines])\n",
    "X_test = pad_sequences(X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "y_test = test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vali, y_train, y_vali = train_test_split(raw_X_train, raw_y_train, random_state = random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "y_train_onehot = np_utils.to_categorical(y_train)\n",
    "y_vali_onehot = np_utils.to_categorical(y_vali)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "del embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights = [embeddings_matrix],\n",
    "                            trainable=False, name='embedding_layer',\n",
    "                            mask_zero=True))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(lstm_hidden_dim, return_sequences=False, name='lstm_layer',\n",
    "                    \n",
    "                    kernel_regularizer =tf.keras.regularizers.L2(l2=1e-3))))\n",
    "    model.add(Dropout(rate=0.8, name='dropout'))\n",
    "    model.add(Activation(activation='relu', name='activation_1'))\n",
    "    model.add(Dense(n_classes, activation='softmax', name='output_layer'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model trained over to four-classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = lstm_model(n_classes=4)\n",
    "basic_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, None, 300)         8835600   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 9,157,204\n",
      "Trainable params: 321,604\n",
      "Non-trainable params: 8,835,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(basic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 28s 62ms/step - loss: 0.9417 - accuracy: 0.7311 - val_loss: 0.7629 - val_accuracy: 0.7372\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 17s 56ms/step - loss: 0.7512 - accuracy: 0.7469 - val_loss: 0.6993 - val_accuracy: 0.7476\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 0.6780 - accuracy: 0.7570 - val_loss: 0.6919 - val_accuracy: 0.7597\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.6353 - accuracy: 0.7719 - val_loss: 0.6028 - val_accuracy: 0.7748\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 0.5910 - accuracy: 0.7896 - val_loss: 0.6041 - val_accuracy: 0.7901\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.5656 - accuracy: 0.8022 - val_loss: 0.5642 - val_accuracy: 0.8048\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 18s 59ms/step - loss: 0.5417 - accuracy: 0.8132 - val_loss: 0.5554 - val_accuracy: 0.8125\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.5458 - accuracy: 0.8148 - val_loss: 0.5829 - val_accuracy: 0.8143\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 0.5124 - accuracy: 0.8259 - val_loss: 0.5000 - val_accuracy: 0.8358\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 0.4954 - accuracy: 0.8319 - val_loss: 0.5419 - val_accuracy: 0.8258\n"
     ]
    }
   ],
   "source": [
    "history = basic_model.fit(X_train, y_train_onehot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch,\n",
    "          validation_data=(X_vali, y_vali_onehot))\n",
    "# basic_model.save(save_path+\"basic_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Relatedness Accuracy is 0.8571285642821411\n",
      "The Opinion Accuracy is 0.44963503649635034\n"
     ]
    }
   ],
   "source": [
    "# Accuracies on the validation set\n",
    "preds_vali = np.argmax(basic_model.predict(X_vali), axis = -1)\n",
    "true_valid = np.argmax(y_vali_onehot, axis = -1)\n",
    "\n",
    "total_relatedness = len(true_valid)\n",
    "total_opinion = 0\n",
    "correct_relatedness = 0\n",
    "correct_opinion = 0\n",
    "\n",
    "for i in range(len(true_valid)):\n",
    "    true = true_valid[i]\n",
    "    pred = preds_vali[i]\n",
    "    if true==3:\n",
    "        if pred==3:\n",
    "            correct_relatedness+=1\n",
    "    else:\n",
    "        total_opinion+=1\n",
    "        if pred!=3:\n",
    "            correct_relatedness+=1\n",
    "        if pred==true:\n",
    "            correct_opinion+=1\n",
    "\n",
    "print(f\"The Relatedness Accuracy is {correct_relatedness/total_relatedness}\")\n",
    "print(f\"The Opinion Accuracy is {correct_opinion/total_opinion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     4     |     0     |    39     |   1860    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     2     |     0     |    17     |    678    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     6     |     0     |    253    |   4205    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    21     |     0     |    838    |   17490   |\n",
      "-------------------------------------------------------------\n",
      "Score: 4645.5 out of 11651.25\t(39.87125844866431%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.87125844866431"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = basic_model.predict(X_test)\n",
    "outputs = [int_to_stance[np.argmax(p, axis = -1)] for p in preds]\n",
    "report_score(actual_test_stances,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relatedness Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agree, disagree, discuss are 1, unrelated is 0\n",
    "int_to_relatedness={0:1,1:1,2:1,3:0}\n",
    "str_to_relatedness = {'unrelated':0 , 'related':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_y_train = y_train.apply(lambda x: int_to_relatedness[x])\n",
    "relatedness_y_train_onehot = np_utils.to_categorical(relatedness_y_train)\n",
    "\n",
    "relatedness_y_vali = y_vali.apply(lambda x: int_to_relatedness[x])\n",
    "relatedness_y_vali_onehot = np_utils.to_categorical(relatedness_y_vali)\n",
    "\n",
    "relatedness_y_test = y_test.copy()\n",
    "relatedness_y_test = relatedness_y_test.apply(lambda x: int_to_relatedness[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_model = lstm_model(n_classes=2)\n",
    "relatedness_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 27s 64ms/step - loss: 0.6716 - accuracy: 0.7466 - val_loss: 0.5344 - val_accuracy: 0.7592\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 0.5025 - accuracy: 0.7768 - val_loss: 0.5657 - val_accuracy: 0.7405\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.4789 - accuracy: 0.7907 - val_loss: 0.5159 - val_accuracy: 0.7764\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.4500 - accuracy: 0.8055 - val_loss: 0.4420 - val_accuracy: 0.7990\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.4101 - accuracy: 0.8257 - val_loss: 0.4027 - val_accuracy: 0.8320\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.3910 - accuracy: 0.8389 - val_loss: 0.4091 - val_accuracy: 0.8368\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.3786 - accuracy: 0.8475 - val_loss: 0.3750 - val_accuracy: 0.8476\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.3507 - accuracy: 0.8632 - val_loss: 0.4060 - val_accuracy: 0.8498\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.3732 - accuracy: 0.8563 - val_loss: 0.3679 - val_accuracy: 0.8647\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 17s 56ms/step - loss: 0.3330 - accuracy: 0.8735 - val_loss: 0.3472 - val_accuracy: 0.8716\n"
     ]
    }
   ],
   "source": [
    "history = relatedness_model.fit(X_train, relatedness_y_train_onehot ,batch_size=batch_size,epochs=epoch, \n",
    "                                validation_data=(X_vali, relatedness_y_vali_onehot))\n",
    "preds = relatedness_model.predict(X_test)\n",
    "outputs = [np.argmax(p, axis = -1) for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6708771101404792"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(relatedness_y_test, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinion Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset that all unrelated column is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_index = train_df[train_df['Stance']==3].index\n",
    "opinion_train_df = train_df.drop(train_df[train_df['Stance']==3].index)\n",
    "\n",
    "test_drop_index = test_df[test_df['Stance']==3].index\n",
    "opinion_test_df = test_df.drop(test_df[test_df['Stance']==3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train_headline = [text_to_word_sequence(clean(head)) for head in opinion_train_df['Headline']]\n",
    "opinion_train_bodies = [text_to_word_sequence(clean(body)) for body in opinion_train_df['articleBody']]\n",
    "\n",
    "opinion_test_headline = [text_to_word_sequence(clean(head)) for head in opinion_test_df['Headline']]\n",
    "opinion_test_bodies = [text_to_word_sequence(clean(body)) for body in opinion_test_df['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_train_lines = [] \n",
    "for i in range(len(opinion_train_headline)):\n",
    "    headline =  opinion_train_bodies[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    opinion_train_lines.append(newline)\n",
    "\n",
    "opinion_test_lines = [] \n",
    "for i in range(len(opinion_test_headline)):\n",
    "    headline =  opinion_test_bodies[i]\n",
    "    body = clean_train_bodies[i]\n",
    "    newline = headline+body\n",
    "    opinion_test_lines.append(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13427"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_train = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in opinion_train_lines])\n",
    "opinion_raw_X_train = pad_sequences(opinion_X_train, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "opinion_raw_y_train = opinion_train_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_test = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_length]) for seq in opinion_test_lines])\n",
    "opinion_raw_X_test = pad_sequences(opinion_X_test, maxlen=max_sent_length, padding='post', truncating='post')\n",
    "opinion_y_test = opinion_test_df['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to onehot\n",
    "opinion_y_train_onehot = np_utils.to_categorical(opinion_raw_y_train)\n",
    "opinion_y_test_onehot = np_utils.to_categorical(opinion_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_X_train, opinion_X_vali, opinion_y_train, opinion_y_vali = train_test_split(opinion_raw_X_train, opinion_y_train_onehot, random_state=random_state, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_model = lstm_model(n_classes=3)\n",
    "opinion_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "84/84 [==============================] - 15s 97ms/step - loss: 1.1238 - accuracy: 0.6560 - val_loss: 0.8408 - val_accuracy: 0.6850\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.8068 - accuracy: 0.6963 - val_loss: 0.7353 - val_accuracy: 0.7368\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.7319 - accuracy: 0.7182 - val_loss: 0.6880 - val_accuracy: 0.7278\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 5s 56ms/step - loss: 0.6754 - accuracy: 0.7491 - val_loss: 0.6640 - val_accuracy: 0.7349\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.6378 - accuracy: 0.7567 - val_loss: 0.5891 - val_accuracy: 0.7643\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.6402 - accuracy: 0.7541 - val_loss: 0.5964 - val_accuracy: 0.7733\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 5s 56ms/step - loss: 0.5780 - accuracy: 0.7761 - val_loss: 0.5717 - val_accuracy: 0.7785\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.5621 - accuracy: 0.7888 - val_loss: 0.5459 - val_accuracy: 0.7852\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 5s 57ms/step - loss: 0.5390 - accuracy: 0.7950 - val_loss: 0.5469 - val_accuracy: 0.7833\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 5s 57ms/step - loss: 0.5312 - accuracy: 0.7963 - val_loss: 0.5211 - val_accuracy: 0.7937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved/opinion_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved/opinion_model\\assets\n"
     ]
    }
   ],
   "source": [
    "history = opinion_model.fit(opinion_X_train, opinion_y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(opinion_X_vali, opinion_y_vali))\n",
    "basic_model.save(save_path+\"opinion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5995186862967158"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = opinion_model.predict(opinion_raw_X_test)\n",
    "outputs = [np.argmax(p, axis = -1) for p in preds]\n",
    "accuracy_score(opinion_y_test, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Relatedness model and Opinion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cascaded_model:\n",
    "    def __init__(self,relatedness_model, opinion_model):\n",
    "        self.relatedness_model = relatedness_model\n",
    "        self.opinion_model = opinion_model\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        prediction = relatedness_model.predict(X_test)\n",
    "        prediction = [np.argmax(p, axis = -1) for p in prediction]\n",
    "        for i in tqdm(range(len(prediction))):\n",
    "            relatedness = prediction[i]\n",
    "            if relatedness == 1: #related\n",
    "                opinion = opinion_model.predict(np.array([X_test[i]]))\n",
    "                opinion = np.argmax(opinion, axis = -1)\n",
    "                \n",
    "                prediction[i] = int(opinion)\n",
    "            else:\n",
    "                prediction[i] = 3\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9995/9995 [01:36<00:00, 103.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stance accuracy of cascaded model is 0.832216108054027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cascaded = cascaded_model(relatedness_model, opinion_model)\n",
    "preds_vali = cascaded.predict(X_vali)\n",
    "true_valid = np.argmax(y_vali_onehot, axis = -1)\n",
    "print(f\"The stance accuracy of cascaded model is {accuracy_score(true_valid,preds_vali)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8c2f1beb2e40a9a38baaa548f28e428769bf26bad7d7d25cc139235a548e0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
